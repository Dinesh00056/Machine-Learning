{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2638fb6-162b-47e2-8607-5f8e74bb6804",
   "metadata": {},
   "source": [
    "# Ensemble learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f73601d5-bbcf-4f62-8cf1-1975b1e97c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble learning is a machine learning technique that envolve combining multiple individual models know as base learner to make prediction and decision.\n",
    "# The main idea behind ensemle learning is that by combining the prediction of multiple models the overall performance can be improved compared to using \n",
    "# a single model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb1c464-2039-4497-92ad-dcda44812189",
   "metadata": {},
   "source": [
    "# Techniques------>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "000fdb8c-ce94-4370-9af4-70d1c73e175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1). Bagging\n",
    "# 2). Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75b03b7d-59e1-4e29-8f62-46fd375b40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Bagging--> Bagging stands for Bootstrap aggregating it involves creating multiple subsets of the training data through bootstraping\n",
    "# training a separate a baselearner on each subset and combining there predicition the most common example of bagging is random forest algorithm\n",
    "# which combines multiple decision tree.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32744f05-57ee-46c3-93b7-d0a9b1325125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Boosting ---> Boosting is an iterative ensemble method that focus on training weak learner sequentially and giving more importance to the instance \n",
    "# that were misclassified by previous learners. In boosting each base learner is trained to correct the mistakes made by previous learner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107e4496-f14c-466d-804a-0925d7b2df2c",
   "metadata": {},
   "source": [
    "# Advantages of Ensemble Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8fd845f-1a94-45b5-9aaf-1d9c0f193efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Improved performance:- Ensemble method can offer achieve better performance than individual models specially when the base learner are diverse and \n",
    "# complementary \n",
    "# 2) Robustness:- Ensemble learning can me more robust to noisy data ensemble learning and outliers because errors made by individual models can be compensating others \n",
    "# 3) Reducing overfitting:- Ensemble method can help reduce overfitting as the combination of multiple orders reducing the risk of relies to heavily on a \n",
    "# signal model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9237e2b4-99d7-4f93-8d02-83cb48094155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
    "# Initialize a list to store the base Learners\n",
    "base_learners=[]\n",
    "#number of base learners(decision trees)\n",
    "num_base_learners= 10\n",
    "# Train the base learners\n",
    "for i in range(num_base_learners):\n",
    "    # Create a bootstrap sample of the training data\n",
    "    bootstrap_indices = np.random.choice(len(X_train), size=len(X_train),\n",
    "                                        replace=True)\n",
    "X_bootstrap = X_train[bootstrap_indices]\n",
    "y_bootstrap = y_train[bootstrap_indices]\n",
    "\n",
    "# Create and train a base learner (Random Forest)\n",
    "base_learner=RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "base_learner.fit(X_bootstrap, y_bootstrap)\n",
    "    # Add the trained base Learner to the List\n",
    "base_learners.append(base_learner)\n",
    "\n",
    "# Make predictions with each base learner\n",
    "base_predictions = []\n",
    "for base_learner in base_learners:\n",
    "   y_pred = base_learner.predict(X_test)\n",
    "   base_predictions.append(y_pred)\n",
    "#combine the prediction using the majority voting\n",
    "ensemble_predictions = np.round(np.mean(base_predictions, axis=0))\n",
    "\n",
    "# Calculate the accuracy of the ensemble predictions\n",
    "accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "print(\"Ensemble Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
