{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df990a20-fc84-46d7-8f40-e4e42355605f",
   "metadata": {},
   "source": [
    "# Support Vector Machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ece56dbf-514e-47ac-8f0c-4f9b8f16e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine is a supervised machine learning algorithm which is used for both classification and regression task it is particular affective \n",
    "# in dealing with complex and high dimensional dataset the fundamental principal of svm is to find an optimal hyperplane that maximly separate different \n",
    "# classes in the input space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea73083b-72b5-4b2d-aebb-c7e11a7abf9e",
   "metadata": {},
   "source": [
    "# How SVM Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8854502b-860c-44d0-b5c5-a953fff0a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) SVM requires labelled training data consisting of input feature and corresponding class labels each data point is represented as feature vector where\n",
    "# each feature describes a particular characteristics of the data point when the datapoint should be preprocessed and scaled to ensure the feature are on\n",
    "# similar skills typically between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39924f05-7a70-44c0-93a6-f2838f116322",
   "metadata": {},
   "source": [
    "# Hyperplane and Margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "289628db-dfac-4392-8d7d-ece56c4e9bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM aims to find a hyperplane that best separates the different classes in the feature space in a binary classification problem the hyperplane is a line\n",
    "# in a 2D space or a hyperplane in a higher dimensional space.\n",
    "# SVM seeks to maximize the margin which is the distance between the hyperplane and the nearest datapoint from each class the points on the margin are known\n",
    "# as support vectors as they play crocial role in defining the decision boundary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd51bb-8e6f-42d2-911e-80a15ff0dc95",
   "metadata": {},
   "source": [
    "# Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "458befa6-e21e-49bb-964d-10b10c573109",
   "metadata": {},
   "outputs": [],
   "source": [
    "1# The linear svm finds a linear hyperplane that separates the classes the goal is to find the hyperplane that maximize the margin while minimizing the \n",
    "# misclassification of training example this can be formulated as an optimization problem with the objective of minimizing the waves of the hyperplane \n",
    "# subject to the constant that all training example lies on the correct side of the hyperplane \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b35b135-377c-4a36-8077-8b3702d311f7",
   "metadata": {},
   "source": [
    "# Non Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c5453ae-80cc-4bea-9dff-30f9979b42a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In cases where data is not linear separable svm uses a technique called the kernel trick the kernel trick maps the original input space into a higher \n",
    "# dimensional feature space where the datapoints can be linear separable \n",
    "\n",
    "# Training------------->\n",
    "# SVM training involves finding the optimal hyperplane or decision boundary thats separates the classes THe optimization problem is typically solved using \n",
    "# methods such as quardatic programming or sequential minimal optimization the process involves solving for the weights of the hyperplane and the bayes term\n",
    "# which define the decision boundary the objective is to minimize the regularization term while ensuring that the training example are correctly classified\n",
    "\n",
    "# Prediction...............>\n",
    "# Once the svm model is trained it can be used to predict the class label of the new unseen datapoints this algorithm computes the distance from test point \n",
    "# to the decision boundary and the predicted class labels is determined based on with side of the decision boundary the point lies the decision function\n",
    "# can also provide a confident score or probability estimate for the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa6e6f-3a3e-426d-af13-4e2b1a67f015",
   "metadata": {},
   "source": [
    "# Advantages Of SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51512475-5de3-4096-98bd-ff884545829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) It is affective in High dimensional space\n",
    "# 2) It is robust againts overfitting due to the margin maximize principal\n",
    "# 3) It is versatile through thee used of different kernel function \n",
    "# 4) It can handle both linear and non linear classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a856d50f-b024-4e15-858e-97ca624fe7ce",
   "metadata": {},
   "source": [
    "# Limitation of SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2ae5fc4-c8ed-4e46-8b9c-d9c8642aa9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) It is computatinsly expensive for large dataset \n",
    "# 2) It requires proper selection of hyperparameters such as the regulaization parameter and the kernel parameter\n",
    "# 3) It is difficult to interprit the learn model compared to simpler algorithm like logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58250444-4884-4aff-9e6a-551105576601",
   "metadata": {},
   "source": [
    "# How i select which kernel i have to use for which data in SVM algorithm---->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e5fdfe9-126d-4905-b84b-fd27898d1bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When selecting the kernel depends upon the data and the problem we are try to solve...\n",
    "# 1) Linear kernel--> It is suitable for lineare separable data iy works well when there is a clear linear boundary between classes\n",
    "# 2) Polynominal Kernel--> It maos the data into a higher dimensional space using the polynominla space. ot is usedful when the decision boundary is curve\n",
    "# and as higher degree of complexity. the degree of the polynominal which determines the complexity can be specified \n",
    "# 3) RBI(Gaussian kernel)--> The Gaussian kernel maps the data into an infinity dimensional space it is suitable for non linearly separable data and works \n",
    "# well when the decision boundary is complex it is popular choice due to its flexibiity and ability to capture intricate pattern \n",
    "# 4) Sigmoid Kernel --> This kernel maps the data into an higher dimensional space using the sigmoid function it is useful when the decision boundary is \n",
    "# S-shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c908820-b398-4c4e-ac36-efb92b526a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f6d4a82-d2aa-46a8-9415-1998ab3243be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Kernel Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "x,y=make_classification(n_samples=100,n_features=2,n_informative=2,n_redundant=0,random_state=42)\n",
    "#Split the data into taining and test sets\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n",
    "# Linear Kernel\n",
    "linear_svc=svm.SVC(kernel=\"linear\")\n",
    "linear_svc.fit(x_train,y_train)\n",
    "linear_predictions=linear_svc.predict(x_test)\n",
    "linear_accuracy=accuracy_score(y_test,linear_predictions)\n",
    "print(\"Linear Kernel Accuracy:\",linear_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e6fa035-2e20-4910-8744-76bf12bfc931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Kernel Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Polynomial Kernel\n",
    "poly_svc=svm.SVC(kernel=\"poly\",degree=3).0\n",
    "poly_svc.fit(x_train,y_train)\n",
    "poly_predictions=poly_svc.predict(x_test)\n",
    "poly_accuracy=accuracy_score(y_test,poly_predictions)\n",
    "print(\"Polynomial Kernel Accuracy:\",poly_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395bc24-03f4-4e78-8bd9-3e10ef0bc87d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
